{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Hands on Classification\n",
    "## Minería de Datos 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Selección de Datos\n",
    "En este taller utilizaremos un dataset \"bien portado\", es decir, que no requiere mucho preprocesamiento (i.e. imputación, control de NaNs, variables no númericas, etc.). La idea es que usted se pueda concentrar en las métricas de selección de clasificadores, más que en el preprocesamiento de los datos. Para esto, cargue el dataset llamado `heart.csv` que puede encontrar en el aula (junto a este archivo). Este dataset contiene 303 pacientes con 13 características (resultados de exámenes o datos fisiológicos):\n",
    "> 1. age \n",
    "> 2. sex \n",
    "> 3. chest pain type (4 values) \n",
    "> 4. resting blood pressure \n",
    "> 5. serum cholestoral in mg/dl \n",
    "> 6. fasting blood sugar > 120 mg/dl\n",
    "> 7. resting electrocardiographic results (values 0,1,2)\n",
    "> 8. maximum heart rate achieved \n",
    "> 9. exercise induced angina \n",
    "> 10. oldpeak = ST depression induced by exercise relative to rest \n",
    "> 11. the slope of the peak exercise ST segment \n",
    "> 12. number of major vessels (0-3) colored by flourosopy \n",
    "> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "En la característica 14 encontrara el `target`, que corresponde a 1 si el paciente tiene una affección cardiaca y 0 si que está sano.\n",
    "\n",
    "Como de costumbre, recuerde describir los datos utilizando pandas para una mayor comprensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "df_heart = pd.read_csv('heart.csv')\n",
    "df_heart.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.366337</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>131.623762</td>\n",
       "      <td>246.264026</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>149.646865</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.399340</td>\n",
       "      <td>0.729373</td>\n",
       "      <td>2.313531</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.082101</td>\n",
       "      <td>0.466011</td>\n",
       "      <td>1.032052</td>\n",
       "      <td>17.538143</td>\n",
       "      <td>51.830751</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.525860</td>\n",
       "      <td>22.905161</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>1.022606</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.498835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
       "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
       "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
       "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
       "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
       "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
       "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
       "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
       "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
       "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
       "\n",
       "             thal      target  \n",
       "count  303.000000  303.000000  \n",
       "mean     2.313531    0.544554  \n",
       "std      0.612277    0.498835  \n",
       "min      0.000000    0.000000  \n",
       "25%      2.000000    0.000000  \n",
       "50%      2.000000    1.000000  \n",
       "75%      3.000000    1.000000  \n",
       "max      3.000000    1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heart.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREGUNTA**: observando los estadísticos del dataset, está de acuerdo de que este dataset es \"bien portado\" (i.e., sin datos faltantes, datos balanceados, datos numéricos, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de ver los datos de las 14 columnas y las estadísticas, estoy de acuerdo ya que son en su mayoria números y no nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Evaluación de Classificadores\n",
    "\n",
    "Considere los siguientes clasificadores\n",
    "1. Nearest Neighbors Classifier with 3 neighbors\n",
    "1. Support Vector Machine with Linear Kernel with $C=0.03$\n",
    "1. Support Vector Machine with Radial Basis Function with C=1.0, $\\gamma=1.5$\n",
    "1. Gaussian Process Classifier with a squared exponential kernel of $\\sigma = 1.0$ and amplitude also $1.0$\n",
    "1. Decision Tree with maximum depth of 6\n",
    "1. Random Forest with maximum depth of 6 and 10 estimators \n",
    "1. Artificial Neural Network with $\\alpha = 1$\n",
    "1. AdaBoost\n",
    "1. Naive Bayes\n",
    "1. Quadratic Discriminant Analysis\n",
    "\n",
    "\n",
    "Evalue el desempeño de los clasificadores\n",
    "* Seleccione 25% de sus datos de forma aleatoria como conjunto de testing, y entrene cada clasificador.\n",
    "* Imprima la curva ROC de cada clasificador y reporte su accuracy, F1-score y AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e72d429c78c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mds_cnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m# preprocess dataset, split into training and test part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1944x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "# for splitting data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# standardize features removing the mean and scaling to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# data sets\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "# multilayer perceptron classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# K nearest neighbors classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Support vector machine, C-support vector classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Gaussian process classifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "# radial basis function kernel (squared exponential kernel)\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Ensemble classifiers: Random forest and AdaBoost\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Quadratic Discriminant Analysis classifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "\n",
    "# different classifiers\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.03, probability = True),\n",
    "    SVC(gamma=1.5, C=1, probability = True),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
    "    DecisionTreeClassifier(max_depth=6),\n",
    "    RandomForestClassifier(max_depth=6, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "\n",
    "# linearly separable datatest\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "# all different datasets\n",
    "datasets = [df_train= df_heart.sample(round((len(df_heart))*0.25))]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 15))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # test_size is 40% of the data\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.25,random_state=42)\n",
    "                \n",
    "    # create grid to evaluate classifiers\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(2 * len(datasets), len(classifiers) + 1, i + ds_cnt * (len(classifiers) + 1))\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "    # Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(2 * len(datasets), len(classifiers) + 1, i + ds_cnt * (len(classifiers) + 1))\n",
    "\n",
    "        # fit the model using the training set\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # compute the mean accuracy of the classifier\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "\n",
    "        # plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                   edgecolors='k', alpha=0.6)\n",
    "\n",
    "        # set limits\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "\n",
    "        # remove ticks\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "\n",
    "        # write title only for the first data set for each classifier\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "\n",
    "        # plot the accuracy score\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                size=15, horizontalalignment='right')\n",
    "\n",
    "        # compute ROC curve\n",
    "        y_test_pred = clf.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_pred, pos_label=1)\n",
    "        auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "        acc = metrics.accuracy_score(y_test, y_test_pred >= 0.5)\n",
    "        f1 = metrics.f1_score(y_test, y_test_pred >= 0.5)\n",
    "        ax = plt.subplot(2 * len(datasets), len(classifiers) + 1, i + (ds_cnt + 1) * (len(classifiers) + 1))\n",
    "        ax.set_xlim(-.05, 1.05)\n",
    "        ax.set_ylim(-.05, 1.05)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.text(0.95, 0.3, \"Acc: %.2f\" % acc, ha = 'right')\n",
    "        ax.text(0.95, 0.2, \"F1-score: %.2f\" % f1, ha = 'right')\n",
    "        ax.text(0.95, 0.1, \"AUC: %.2f\" % auc, ha = 'right')\n",
    "        ax.plot(fpr, tpr, lw = 5)\n",
    "        idx = np.argmin(np.abs(thresholds - 0.5))\n",
    "        ax.scatter(fpr[idx], tpr[idx], marker = 'o', c = 'r')\n",
    "\n",
    "        # counter \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREGUNTA**: ¿Qué comportamiento observa en las métricas para distintos muestreos del conjunto testing? ¿Por qué este análysis sería considerado \"trampa\" en un caso real?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Selección de Modelo\n",
    "Seleccione ahora un conjunto de testing del 30% de los datos, usando una semilla aleatoria fija (por ejemplo 42).\n",
    "Realice una validación cruzada con un K-Fold de 20 y reporte los promedios de accuracy, F1-score y AUC utilizando sólo los conjuntos de validación (no testing!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREGUNTA**: Haga un ranking de los 3 mejores clasificadores para este problema (con una semilla fija la selección del conjunto de training). Justifique su elección utilizando las métricas computadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Selección de Parámetros\n",
    "Seleccione uno de los 3 clasificadores de su ranking. Seleccione un parámetro del modelo y haga un [gridsearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) (o puede hacerlo manualmente también) para elegir un parámetro adecuado para su clasificador. Muestre sus resultados utilizando un gráfico con el parámetro como la abcisa (eje x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREGUNTA**: ¿Tiene sentido el resultado considerando el sigificado del parámetro dentro del contexto de su clasificador? Justifique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Reporte de Resultados\n",
    "Entrene ahora su clasificador con el 70% de los datos (training) utilizando el **mejor** parámetro encontrado mediante cross-validation, y otra instancia con el parámetro original entregado (o el que venía por defecto). Evalúe en el restante 30% (testing) usando los 3 criterios (i.e., accuracy, F1-score y AUC) para ambos casos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREGUNTA FINAL**: ¿Le fue útil realizar el ejercicio de cross-validation para obtener mejores resultados?. Comente brevemente porqué hacer cross-validation \"no es trampa\", haciendo referencia a lo comentado en la sección 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
